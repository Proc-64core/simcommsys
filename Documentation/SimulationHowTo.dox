/*!
\file

Copyright (c) 2013 Johann A. Briffa

This file is part of SimCommSys.

SimCommSys is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

SimCommSys is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with SimCommSys.  If not, see <http://www.gnu.org/licenses/>.


\page simulation How to set up and run a simulation
\author   Stephan Wesemeyer
$Id$

\todo Update formatting for doxygen and review.


Assumption:
-----------

1) You have a working svn directory:/path_to_svn_dir/ with the following
projects checked out

   Projects/ECW
   Programming
   Scripts


Pre-requisites:
---------------

1) Ensure that the ECW, Programming and Scripts folders are up to date by using
svn up in each of them

2) Ensure that you have /path_to_svn_dir/Scripts/Python, SimCommsys and Condor
in your search path, eg add

   #add the Python folder to my Path
   PATH="/path_to_svn_dir/Scripts/Python:/path_to_svn_dir/Scripts/SimCommsys:
   /path_to_svn_dir/Scripts/Condor:$PATH"

to your .bashrc file in your home directory

3) In Programming/branches/yourBranch run

   make install

This will create a bin.Arch/ directory in your home directory containing the
binaries for your CPU architecture (eg bin.i686 for 32bit CPUs) within it the
binaries adhere to the following naming convention: name.yourBranch.Debug and
name.yourBranch.Release, eg simcommsys.jab.Release

4) Ensure you have bin.Arch in your search path, eg add

   #add bin.i686 directory to my Path
   PATH="$HOME/bin.i686:$PATH"

to your .profile file in your home directory


Set-up the simulation:
----------------------

1) cd to /path_to/ECW/Results

2) Set up a new directory for your simulation by copying the provided template,
eg

   svn cp TemplateSimulation YourSimulation

3) Copy your systems that you want to simulate into the Systems folder (either
using svn cp if they are under version control or just cp and svn add if they
are not)

4) Run the Makefile to ensure all the appropriate simulator files are created
from the system files

5) commit your files to svn


Run the simulation locally
(client-server mode but not on the cluster)
---------------------------------------------

There are 2 simulation modes: abstract and conventional

"abstract" uses a logarithmic scale (ie the steps between simulations are
obtained using a multiplier)

"conventional" uses a standard interval by which the values are
increased/decreased.

For "abstract" simulations:

1) cd to YourSimulation/

2) run the following command to generate data points for a logarithmic scale

simulate-batch.sh abstract <tag> <port> "[<start> [<mult> [<end> [<floor> [<tol>
[<conf>]]]]]]" systemName.txt

where

abstract indicates a logarithmic scale

<tag> is the name of your branch, eg jab
<port> is the port number on which the server should listen

The parameters in quotes have some default values and hence can be omitted if
the default is correct

<start>(0.5) : is the value from which the simulation should start (eg snr in dB
or insertion/deletion probability)

<mult>(0.8) : is the value with which the simulator will multiply to obtain the
next value at which to start a new simulation

<end>(1e-5) : is the end value at which the simulation will finish (unless the
floor value has been reach previously)

<floor>(1e-5) : is the error level that once surpassed will actually terminate
the simulation even if the end value has not been reached.

<tol>(0.20) : is the interval around the measured value in which the true result
should lie

<conf>(0.80) : is the confidence level that the result lies within the tolerance
interval

The tol and conf parameters determine how long the monte carlo simulation is
going to run. The lower the tolerance interval and the higher the required
confidence level is set the longer the simulation will need to run to achieve
that. In order to obtain results that are publishable, these values should be
set to tol=0.05 (ie the true result will be within +/-5% of the measured result)
and conf=0.99 (ie we are 99% certain that the true result lies within that
tolerance level).

For tol=0.05 and conf=0.99, if the measure value is 1 then we are 99% confident
that the true result is between 0.95 and 1.05.

For quick and dirty results and hence quicker results which are still indicative
the default values are acceptable

systemName.txt is the name of the system to simulate which must be under the
directory Simulators

3) You can now start as many client process as you have CPUs by executing:

   simcommsys.<tag>.Release -e localhost:<port>

where

   <tag> is the name of your branch, eg jab
   <port> is the port number you specified in step 2

Note that the simulator runs in a screen session, type

screen -ls to check that a session is running and screen -r <id> (where <id> is
the screen session) to check it is running.

For "conventional" simulations:

1) cd to YourSimulation/

2) run the following command to generate data points for a logarithmic scale

simulate-batch.sh conventional <tag> <port> "[<start> [<step> [<end> [<floor>
[<tol> [<conf>]]]]]]" systemName.txt

where

abstract indicates a logarithmic scale

<tag> is the name of your branch, eg jab
<port> is the port number on which the server should listen

The parameters in quotes have some default values and hence can be omitted if
the default is correct

<start>(0.0) : is the value from which the simulation should start (eg snr in dB
or insertion/deletion probability)

<step>(0.25) : is the value which the simulator will add to obtain the next
value at which to start a new simulation

<end>(3.0) : is the end value at which the simulation will finish (unless the
floor value has been reach previously)

<floor>(1e-5) : is the error level that once surpassed will actually terminate
the simulation even if the end value has not been reached.

<tol>(0.20) : is the interval around the measured value in which the true result
should lie

<conf>(0.80) : is the confidence level that the result lies within the tolerance
interval

The tol and conf parameters determine how long the monte carlo simulation is
going to run. The lower the tolerance interval and the higher the required
confidence level is set the longer the simulation will need to run to achieve
that. In order to obtain results that are publishable, these values should be
set to tol=0.05 (ie the true result will be within +/-5% of the measured result)
and conf=0.99 (ie we are 99% certain that the true result lies within that
tolerance level).

For tol=0.05 and conf=0.99, if the measure value is 1 then we are 99% confident
that the true result is between 0.95 and 1.05.

For quick and dirty results and hence quicker results which are still indicative
the default values are acceptable

systemName.txt is the name of the system to simulate which must be under the
directory Simulators

3) You can now start as many client process as you have CPUs by executing:

   simcommsys.<tag>.Release -e localhost:<port>

where

   <tag> is the name of your branch, eg jab
   <port> is the port number you specified in step 2

Run the simulation on the cluster
----------------------------------

The principles are the same as for the local simulation. The main difference is
the way the jobs are submitted and where the server and client processes are
located. The current convention is to run the server process on Hawker05 and the
client processes on various nodes on the cluster for which tempest101 is the
designated head node.

If there are multiple people running simulations you need to ensure that every
one is allocated a range of ports on Hawker05, eg

   jab: 9999 - 9900
   swe: 9899 - 9800
   hgs: 9799 - 9700

Note that Hawker05 is i686 and the cluster is x86_64, so the source code needs
to be compiled & installed both on hawker05 and tempest101. As hawker05 and
tempest101 share the same home directories so everything that is available to
hawker05 is available to tempest101. Unfortunately, tempest101 does not have an
svn client, so any checkouts/commits need to be done on hawker05.

However, if you have another Linux desktop on the internal network, you share
the same home directory with hawker05 and tempest101. If you have done any
checkouts on your desktop machine, the likelihood is that your svn client is
more recent than the one used on hawker05. This might mean that hawker05 will
refuse to checkout files as the svn metadata was generated by a newer version of
the svn client. If this is the case, you can either use separate working folders
or just ensure that you only ever check code out on your desktop machine.

The server process on hawker is started exactly as before. Please ensure that
you use the right port ranges allocated to you.

To start the clients on the cluster you need to ssh onto tempest101 which is the
designated headnode of the cluster.

Ensure that you have the lastest binaries for tempest by executing make && make
install on tempest.

you can now simply execute:

   submit-simcommsys.sh <tag> <count> <host> <first port> [<last port>]

where

<tag> should be something like stable-99 and refers to the appropriate release
but might just be your branch name, eg jab

<count> is the number of processes you wish to start

<host> this is the host where the  server process is located, usually hawker05

<first port> is the first port on which the server process is listening

<last port> is the last port on which the server process is listening (note that
<first port> must be less than <last port>)

So if you started a number of different server processes on hawker05 which
listen on consecutive ports in ascending order you can allocated the same number
of clients to each of them with the above command.

Most users will probably want to vary the number of clients per server process
and hence will call submit-simcommsys.sh multiple times.

There are several commands that are useful to query/control the cluster:

1) condor_status gives an overview of how many CPUs the cluster has and whether
they have been allocated to run any jobs

2) condor_q shows the current queues of jobs running on the cluster

3) condor_rm <user> removes all jobs owned by <user>

4) condor_rm <condor.proc> removes the given job

*/
